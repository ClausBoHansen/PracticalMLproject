---
title: "Practical Machine Learning Project"
author: "Claus Bo Hansen"
date: "January 3, 2019"
output: html_document
---

<style type="text/css">

h1{
      margin-top: 2em;
  }

</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Purpose
The purpose of this analysis is to predict the class of physical activity.

**Background**

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

**What you should submit**

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.

**Peer Review Portion**

Your submission for the Peer Review portion should consist of a link to a Github repo with your R markdown and compiled HTML file describing your analysis. Please constrain the text of the writeup to < 2000 words and the number of figures to be less than 5. It will make it easier for the graders if you submit a repo with a gh-pages branch so the HTML page can be viewed online (and you always want to make it easy on graders :-).

**Course Project Prediction Quiz Portion**

Apply your machine learning algorithm to the 20 test cases available in the test data above and submit your predictions in appropriate format to the Course Project Prediction Quiz for automated grading.

# Load libraries

```{r, echo = TRUE, warning=FALSE}

library(dplyr, warn.conflicts = FALSE)
library(caret)
library(Boruta)
library(factoextra)
library(corrplot)
library(doParallel)

```


# Get Data

Download data and load into variables.

```{r, echo = TRUE}

# Data sources
URLs <- c("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
          "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")


# Download files and read into data frames
for (URL in URLs) {
  filename <- gsub("^([^/]*/)*(.*)", "\\2", URL)
  varname <- gsub("^([^\\.]*).*", "\\1", filename)
#  download.file(url = URL, filename, method = "auto") # can be commented out when files are downloaded
  assign(varname, read.csv(filename))
}

```


# Clean Data

Remove observations and variables not useful for modelling.

```{r, echo = TRUE}

train_validate <- `pml-training`

# Rows with new_window set to yes are summaries of repetition,
# and should not be included in training/validation
train_validate <- train_validate %>%
  filter(new_window != "yes")

# Columns to be removed
              # Summary columns
regpats <- c("^(kurtosis|skewness|max_|min_|avg_|stddev_|var_|amplitude_)",
              # Not available in "the wild"
             "^(X|user_name|raw_timestamp_part_[12]|cvtd_timestamp|(new|num)_window)$")

# Remove columns matching patterns in regpats
for (regpat in regpats) {
  train_validate <- train_validate[,!grepl(regpat, names(train_validate))]
}

```


# Split into training and validation data sets

Split observations into train data set for model fitting and validate for benchmarking models.

```{r, echo = TRUE}

set.seed(1)

# Create indices for training data, stratify classe
trainIndex <- createDataPartition(train_validate$classe, p=0.8, list = FALSE)

# Split train_validate into data_train and data_validate
data_train <- train_validate[trainIndex,]
data_validate <- train_validate[-trainIndex,]

```



# Feature creation

Create new features/variables, which potentially have better predictive power than the "native" predictors.
I will create new variables, which contain the product of gyros, accel and magment in the three axes.
Example:

accel_PRODUCT_x = accel_belt_x * accel_arm_x * accel_dumbell_x * accel_forearm_x

```{r, echo = TRUE}

# Function to add new features
addfeatures <- function(dataset) {
  metrics <- c("gyros", "accel", "magnet")
  axes <- c("x", "y", "z")
  for (metric in metrics) {
    for (axis in axes) {
      productcolumn <- paste(metric,"_PRODUCT_", axis, sep = "")
      beltcolumn <- paste(metric,"_belt_", axis, sep = "")
      armcolumn <- paste(metric,"_arm_", axis, sep = "")
      dumbbellcolumn <- paste(metric,"_dumbbell_", axis, sep = "")
      forearmcolumn <- paste(metric,"_forearm_", axis, sep = "")
      dataset[,productcolumn] <- as.numeric(dataset[,beltcolumn]) * as.numeric(dataset[,armcolumn]) *
        as.numeric(dataset[,dumbbellcolumn]) * as.numeric(dataset[,forearmcolumn])
    }
  }
  dataset
}

# Add features to data_train and data_validate
data_train <- addfeatures(data_train)
data_validate <- addfeatures(data_validate)


```


# Variable normalization

Normalize (center and scale) variables. Normalization parameters are based on training dataset, same parameters
must be used on validation and test data.

```{r, echo = TRUE}

# Specify which columns to normalize, in this case all but classe
all_cols <- names(data_train)
normalize_cols <- all_cols[!(all_cols %in% c("classe"))]

# Create normalization object, based on training data
normalize_object <- preProcess(data_train[, normalize_cols], method = c("center", "scale"))

# Normalize training and validation data
data_train[, normalize_cols] <- predict(normalize_object, data_train[, normalize_cols])
data_validate[, normalize_cols] <- predict(normalize_object, data_validate[, normalize_cols])



```


# Select variables with predictive power

Use the Boruta package to identify variables correlated with classe.

```{r, echo = TRUE}

# Run Boruta
#boruta_train <- Boruta(classe ~ ., data = data_train)

# Print result
#print(boruta_train)
#plot(boruta_train)

```

None of the predictors can be eliminated by Boruta.


# Principal Component Analysis (PCA)

Investigate if the number of variables can be reduced to fewer Principal Components while still explaining most of the variance.

```{r, echo = TRUE}

# Use all but classe for PCA
all_cols <- names(data_train)
PCA_input <- all_cols[!(all_cols %in% c("classe"))]

# Extract data frame with only predictors (all for PCA)
predictorvars <- data_train %>%
  select(PCA_input)

# Create PCA object
PCAobject <- prcomp(predictorvars)

# Scree plot
fviz_eig(PCAobject)

# Get Eigen values and variance explained
get_eigenvalue(PCAobject)

```

95.3% of variance is explained by the first 30 Principal Components, I will use these first 30 PCs.

```{r, echo = TRUE}

PCsIncluded <- 30

# Calculate PCA columns
PC_train <- as.data.frame(predict(PCAobject, data_train))[,1:PCsIncluded]
PC_validate <- as.data.frame(predict(PCAobject, data_validate))[,1:PCsIncluded]

# Update training and validation data sets to contain only classe and PCs
data_train <-cbind(data_train$classe, PC_train) %>% rename(classe = 'data_train$classe')
data_validate <-cbind(data_validate$classe, PC_validate) %>% rename(classe = 'data_validate$classe')

```



# Check for colinearity between predictors

Many Machine Learning models do not work well with correlated predictors, I need to check for colinearity.

```{r, echo = TRUE}

# Check for colinearity between all but classe
all_cols <- names(data_train)
cor_input <- all_cols[!(all_cols %in% c("classe"))]

# Extract data frame with only predictors (all for PCA)
corvars <- data_train %>%
  select(cor_input)

corrplot(cor(corvars), type = "upper")

```

As expected, there is no colinearirity between the Pricipal Components.



# Model building

Build different models on training set and select the model that works best on validation data.

```{r, echo = TRUE}

# Number of cores allowed for parallel processing
parallelCoreCnt = 8

# Enable parallel
cl <- makePSOCKcluster(parallelCoreCnt)
registerDoParallel(cl)


# Data sets to include in model building
modeldatasets <- c("data_train", "data_validate")

# Function to train model and log result
trainModel <- function(methodtag, methodname, tunegrid = c(), logging = FALSE, method = "repeatedcv", number = 10, repeats = 10){

  trainSetting <- trainControl(method, number, repeats, search = "grid", allowParallel = TRUE)
  
  cat(paste("Training model with method ", method, "\n", sep = ""))
  cat(paste("Number = ", number, "\n", sep = ""))
  cat(paste("Repeats = ", repeats, "\n", sep = ""))


  # Train model on training data
  set.seed(1)
  if (!is.null(tunegrid)) {
      TrainedModel <- train(classe ~ ., method = methodtag, data = data_train, tuneGrid = tunegrid, trControl = trainSetting)
  } else {
    TrainedModel <- train(classe ~ ., method = methodtag, data = data_train, trControl = trainSetting)
  }
  

  # Use model to predict on train and validate data, log results
  for (modeldataset in modeldatasets) {

    cat(paste("\n*** Results on ", modeldataset, ": ***\n\n", sep = ""))

    assign("modeldata", get(modeldataset))
    
    # Create prediction
    Prediction <- predict(TrainedModel, modeldata)
    modeldata <- cbind(modeldata, Prediction)
    
    # Select prediction and case ID for cost estimation
#    modelprediction <- modeldata %>% dplyr::select(c(CAS_ID, Prediction))

    # Log result
#    costestimate <- EstimateAverageCost(modelprediction, logresult = logging, modelname = methodname, datasetname = modeldataset)

  }

  # Return trained model
  TrainedModel
}


# K-Nearest Neighbors

methodtag <- "knn"
methodname <- "K-Nearest Neighbors"
reps <- 1

# Model specific training grid setting
tunegrid <- expand.grid(.k=c(1:5))

TrainedModel <- trainModel(methodtag, methodname, tunegrid, logging = TRUE, repeats = reps)
TrainedModel
plot(TrainedModel, main = paste(methodname, reps, "repeats"))




```


